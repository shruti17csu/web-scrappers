{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://www.moodys.com/credit-ratings/Amazoncom-Inc-credit-rating-600042665'\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elems = soup.find_all(id='ratings')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_requests = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_url = \"https://bitbucket.org/account/signin/?next=/\"\n",
    "result = session_requests.get(login_url)\n",
    "\n",
    "tree = html.fromstring(result.text)\n",
    "authenticity_token = list(set(tree.xpath(\"//input[@name='csrfmiddlewaretoken']/@value\")))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re  \n",
    "q1=\"stock price of\"\n",
    "q = input(\"Enter name\")\n",
    "query=q1+q\n",
    "query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "number_result = 50\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number_result)\n",
    "response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "print(soup.prettify())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1= soup.find(class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "\n",
    "print(r1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "\n",
    "for r in result_div:\n",
    "    # Checks if each element is present, else, raise exception\n",
    "    try:\n",
    "        link = r.find('a', href = True)\n",
    "        title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "        description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "        \n",
    "        # Check to make sure everything is present before appending\n",
    "        if link != '' and title != '' and description != '': \n",
    "            links.append(link['href'])\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "    # Next loop if one element is not present\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "to_remove = []\n",
    "clean_links = []\n",
    "for i, l in enumerate(links):\n",
    "    clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "\n",
    "    # Anything that doesn't fit the above pattern will be removed\n",
    "    if clean is None:\n",
    "        to_remove.append(i)\n",
    "        continue\n",
    "    clean_links.append(clean.group(1))\n",
    "\n",
    "# Remove the corresponding titles & descriptions\n",
    "for x in to_remove:\n",
    "    del titles[x]\n",
    "    del descriptions[x]\n",
    "\n",
    "for i in range(0,len(clean_links)) :\n",
    "    print(titles[i])\n",
    "    print(descriptions[i])\n",
    "    print(clean_links[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re  \n",
    "q = input(\"Enter name\")\n",
    "query=q\n",
    "query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "number_result = 50\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "google_url = \"https://www.fitchratings.com/search?&query=\" + query + \"&num=\" + str(number_result)\n",
    "response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "print(soup.prettify())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "USERNAME = \"<USERNAME>\"\n",
    "PASSWORD = \"<PASSWORD>\"\n",
    "\n",
    "LOGIN_URL = \"https://bitbucket.org/account/signin/?next=/\"\n",
    "URL = \"https://bitbucket.org/dashboard/repositories\"\n",
    "\n",
    "def main():\n",
    "    session_requests = requests.session()\n",
    "\n",
    "    # Get login csrf token\n",
    "    result = session_requests.get(LOGIN_URL)\n",
    "    tree = html.fromstring(result.text)\n",
    "    authenticity_token = list(set(tree.xpath(\"//input[@name='csrfmiddlewaretoken']/@value\")))\n",
    "\n",
    "    # Create payload\n",
    "    payload = {\n",
    "        \"username\": USERNAME, \n",
    "        \"password\": PASSWORD, \n",
    "        \"csrfmiddlewaretoken\": authenticity_token\n",
    "    }\n",
    "\n",
    "    # Perform login\n",
    "    result = session_requests.post(LOGIN_URL, data = payload, headers = dict(referer = LOGIN_URL))\n",
    "\n",
    "    # Scrape url\n",
    "    result = session_requests.get(URL, headers = dict(referer = URL))\n",
    "    tree = html.fromstring(result.content)\n",
    "    bucket_names = tree.xpath(\"//div[@class='repo-list--repo']/a/text()\")\n",
    "\n",
    "    print(bucket_names)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from bs4 import BeautifulSoup as bs\n",
    " \n",
    "with Session() as s:\n",
    "    site = s.get(\"http://quotes.toscrape.com/login\")\n",
    "    print(site.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from bs4 import BeautifulSoup as bs\n",
    " \n",
    "with Session() as s:\n",
    "    site = s.get(\"https://www.moodys.com/login\")\n",
    "    bs_content = bs(site.content, \"html.parser\")\n",
    "    token = bs_content.find(\"input\", {\"name\":\"csrf_token\"})[\"value\"]\n",
    "    login_data = {\"username\":\"admin\",\"password\":\"12345\", \"csrf_token\":token}\n",
    "    s.post(\"https://www.moodys.com/login\",login_data)\n",
    "    home_page = s.get(\"https://www.moodys.com\")\n",
    "    print(home_page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from bs4 import BeautifulSoup as bs\n",
    "with Session() as s:\n",
    "    bs_content = bs(site.content, \"html.parser\")\n",
    "    token = bs_content.find(\"input\", {\"name\":\"csrf_token\"})[\"value\"]\n",
    "    login_data = {\"username\":\"admin\",\"password\":\"12345\", \"csrf_token\":token}\n",
    "    s.post(\"http://quotes.toscrape.com/login\", login_data)\n",
    "    home_page = s.get(\"http://quotes.toscrape.com\")\n",
    "print(home_page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re  \n",
    "\n",
    "q = input(\"Enter name\")\n",
    "query=q\n",
    "query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "number_result = 50\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "google_url = \"https://www.moodys.com/credit-ratings/\" + query + \"-credit-rating\"\n",
    "response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "print(soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter nameubs\n",
      "\u001b[31mUBS AG (Switzerland) - Bank Profile - TheBanks.eu\n",
      "\u001b[34m Long-term credit rating assigned to the bank by Fitch is AA- (very high credit quality)\n",
      "\u001b[31mUBS AG (Switzerland) - Bank Profile - TheBanks.eu\n",
      "\u001b[34m Long-term credit rating assigned to theÂ \n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re  \n",
    "from colorama import Fore, Back, Style \n",
    "\n",
    "q1=\"credit ratinngs of\"\n",
    "q = input(\"Enter name\")\n",
    "query=q1+q\n",
    "query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "number_result = 100\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number_result)\n",
    "response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "\n",
    "links = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "for r in result_div:\n",
    "    # Checks if each element is present, else, raise exception\n",
    "    try:\n",
    "        link = r.find('a', href = True)\n",
    "        title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "        description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "        \n",
    "        # Check to make sure everything is present before appending\n",
    "        if link != '' and title != '' and description != '': \n",
    "            links.append(link['href'])\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "    # Next loop if one element is not present\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "to_remove = []\n",
    "clean_links = []\n",
    "for i, l in enumerate(links):\n",
    "    clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "\n",
    "    # Anything that doesn't fit the above pattern will be removed\n",
    "    if clean is None:\n",
    "        to_remove.append(i)\n",
    "        continue\n",
    "    clean_links.append(clean.group(1))\n",
    "\n",
    "# Remove the corresponding titles & descriptions\n",
    "for x in to_remove:\n",
    "    del titles[x]\n",
    "    del descriptions[x]\n",
    "\n",
    "for i in range(0,len(clean_links)) :\n",
    "    for j in descriptions[i].split(\".\"):\n",
    "        if \"Long-term\" in j:\n",
    "            print(Fore.RED+titles[i])\n",
    "            print(Fore.BLUE+j)\n",
    "        elif \"LT\" in j:\n",
    "            print(Fore.RED+titles[i])\n",
    "            print(Fore.BLUE+j)\n",
    "        #elif \"lt\" in j:\n",
    "           # print(Fore.RED+titles[i])\n",
    "            #print(Fore.BLUE+j)\n",
    "       # elif \"long-term\" in j:\n",
    "           # print(Fore.RED+titles[i])\n",
    "            #print(Fore.BLUE+j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [400]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://api.fitchratings.com/'\n",
    "page = requests.get(URL)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET query missing.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
